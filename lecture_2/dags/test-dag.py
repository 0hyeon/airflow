import requests
import json
import boto3
import pandas as pd
import os
import time
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# ë‚ ì§œ ê³„ì‚° (ì›”ìš”ì¼ì´ë©´ 3ì¼ì¹˜ ê°€ì ¸ì˜¤ê¸°, ì•„ë‹ˆë©´ 1ì¼ì¹˜)
current_date = datetime.now()
current_day_of_week = current_date.weekday()
yesterday = current_date - timedelta(days=1)

from_date = "2025-03-01"
to_date = "2025-03-01"

TOKEN = Variable.get("TOKEN")

HEADERS = {"accept": "application/json", "authorization": f"Bearer {TOKEN}"}


##
URLS = {
    "1pick_view_jobposting_AOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/com.jobkorea.app/in-app-events-retarget/v5?from=2025-03-01&to=2025-03-01&timezone=Asia%2FSeoul&category=standard&event_name=1pick_view_jobposting&additional_fields=device_category",
    # "1pick_view_jobposting_iOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/id569092652/in_app_events_report/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=1pick_view_jobposting",
    # "careercheck_assess_complete_AOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/com.jobkorea.app/in_app_events_report/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=careercheck_assess_complete",
    # "careercheck_assess_complete_iOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/id569092652/in-app-events-retarget/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=careercheck_assess_complete",
}
#
# **S3 ì„¤ì •**
S3_BUCKET = "fc-practice2"
S3_KEY_PREFIX = "apps_flyer_data/"

AWS_ACCESS_KEY = Variable.get("AWS_ACCESS_KEY")
AWS_SECRET_KEY = Variable.get("AWS_SECRET_KEY")
AWS_DEFAULT_REGION = Variable.get("AWS_DEFAULT_REGION")


# **S3 ì €ì¥ í•¨ìˆ˜**
def save_to_s3(records, filename):
    session = boto3.Session(
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_DEFAULT_REGION,
    )
    s3_client = session.client("s3")
    # s3_client = boto3.client("s3")
    file_path = (
        f"{S3_KEY_PREFIX}{filename}_{datetime.now().strftime('%Y-%m-%d')}.parquet"
    )

    try:
        df = pd.DataFrame(records)
        temp_file = "/tmp/temp.parquet"
        df.to_parquet(temp_file, engine="pyarrow", index=False)

        s3_client.upload_file(temp_file, S3_BUCKET, file_path)
        print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{S3_BUCKET}/{file_path}")
    except Exception as e:
        print(f"âŒ Error saving to S3: {e}")


# **API ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ S3ì— ì €ì¥**
def fetch_and_save_data():
    for filename, url in URLS.items():
        try:
            print(f"ğŸ“¡ Fetching data from {url}")
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
            data = response.json()

            if not data:
                print(f"âš ï¸ No data for {filename}, skipping.")
                continue

            print(f"âœ… API ì‘ë‹µ ë°ì´í„°: {data[:5]}")  # ì¼ë¶€ ë°ì´í„° ì¶œë ¥
            print(f"âœ… ë°ì´í„° íƒ€ì…: {type(data)}")

            save_to_s3(data, filename)

        except requests.exceptions.RequestException as e:
            print(f"âŒ Error fetching data from {url}: {e}")


# **Airflow DAG ì„¤ì •**
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2023, 3, 2),
    "retries": 1,
}

with DAG(
    dag_id="daily_s3_storage",
    default_args=default_args,
    schedule_interval="@daily",  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,
) as dag:

    task_fetch_and_save = PythonOperator(
        task_id="fetch_and_save_data",
        python_callable=fetch_and_save_data,
    )
