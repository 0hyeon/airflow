import argparse
import requests
import json
import boto3
import pandas as pd
import os
import time
from datetime import datetime, timedelta
from confluent_kafka import Consumer, Producer

from airflow import DAG
from airflow.operators.python import PythonOperator

# ë‚ ì§œ ê³„ì‚° (ì›”ìš”ì¼ì´ë©´ 3ì¼ì¹˜ ê°€ì ¸ì˜¤ê¸°, ì•„ë‹ˆë©´ 1ì¼ì¹˜)
current_date = datetime.now()
current_day_of_week = current_date.weekday()
yesterday = current_date - timedelta(days=1)
# from_date = yesterday if current_day_of_week != 0 else current_date - timedelta(days=3)
# to_date = yesterday
from_date = "2025-03-01"
to_date = "2025-03-01"

TOKEN = os.getenv("JOBKOREA_TOKEN")
HEADERS = {"accept": "application/json", "authorization": f"Bearer {TOKEN}"}

# **URL ë¦¬ìŠ¤íŠ¸ (ê° URL ë³„ë¡œ ì €ì¥)**
URLS = {
    "1pick_view_jobposting_AOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/com.jobkorea.app/in-app-events-retarget/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=1pick_view_jobposting",
    # "1pick_view_jobposting_iOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/id569092652/in_app_events_report/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=1pick_view_jobposting",
    # "careercheck_assess_complete_AOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/com.jobkorea.app/in_app_events_report/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=careercheck_assess_complete",
    # "careercheck_assess_complete_iOS": f"https://hq1.appsflyer.com/api/raw-data/export/app/id569092652/in-app-events-retarget/v5?from={from_date:%Y-%m-%d}&to={to_date:%Y-%m-%d}&timezone=Asia%2FSeoul&category=standard&event_name=careercheck_assess_complete",
}


# **Kafka Producer ì„¤ì •**
def create_kafka_producer(brokers):
    return Producer({"bootstrap.servers": brokers})


# **Kafka Consumer ì„¤ì •**
def create_kafka_consumer(brokers, topic, group_id="fetch-group"):
    conf = {
        "bootstrap.servers": brokers,
        "group.id": group_id,
        "auto.offset.reset": "earliest",
        "enable.auto.commit": False,
    }
    consumer = Consumer(conf)
    consumer.subscribe([topic])
    return consumer


# **URLë³„ ë°ì´í„° ìˆ˜ì§‘ í›„ Kafkaì— ì „ì†¡**
def stream_to_kafka():
    producer = create_kafka_producer("kafka-service:9092")

    for filename, url in URLS.items():
        try:
            print(f"ğŸ“¡ Fetching data from {url}")
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
            data = response.json()  # JSON ë°ì´í„°

            if not data:
                print(f"âš ï¸ No data for {filename}, skipping.")
                continue

            # **Kafkaë¡œ ì „ì†¡ (URL ë³„ë¡œ ë°ì´í„° ì „ì†¡)**
            for record in data:
                producer.produce(
                    "category-match-out",
                    json.dumps({"filename": filename, "data": record}),
                )
                producer.poll(0)

            producer.flush()
            print(f"âœ… Kafkaë¡œ {len(data)}ê±´ ì „ì†¡ ì™„ë£Œ ({filename})")

        except requests.exceptions.RequestException as e:
            print(f"âŒ Error fetching data from {url}: {e}")


# **Kafka Consumerê°€ URL ë‹¨ìœ„ë¡œ S3ì— ì €ì¥**
def consume_and_save_to_s3():
    consumer = create_kafka_consumer("kafka-service:9092", "category-match-out")
    records_by_filename = {}

    while True:
        msg = consumer.poll(timeout=5.0)
        if msg is None:
            time.sleep(1)
            continue

        message = json.loads(msg.value())
        filename = message["filename"]
        record = message["data"]

        # **íŒŒì¼ëª…ë³„ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ì €ì¥**
        if filename not in records_by_filename:
            records_by_filename[filename] = []

        records_by_filename[filename].append(record)

        # **URL ë‹¨ìœ„ë¡œ S3ì— ì €ì¥ (Kafkaì—ì„œ ë°›ì€ ëª¨ë“  ë°ì´í„° ì €ì¥)**
        if len(records_by_filename[filename]) > 0:
            save_to_s3(
                records_by_filename[filename],
                "fc-practice2",
                "apps_flyer_data/",
                filename,
            )
            records_by_filename[filename] = []  # ì €ì¥ í›„ ì´ˆê¸°í™”


# **S3 ì €ì¥ í•¨ìˆ˜**
def save_to_s3(records, s3_bucket, s3_key_prefix, filename):
    s3_client = boto3.client("s3")
    file_path = (
        f"{s3_key_prefix}{filename}_{datetime.now().strftime('%Y-%m-%d')}.parquet"
    )

    try:
        df = pd.DataFrame(records)
        with open("/tmp/temp.parquet", "wb") as f:
            df.to_parquet(f, engine="pyarrow", index=False)

        s3_client.upload_file("/tmp/temp.parquet", s3_bucket, file_path)
        print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{file_path}")
    except Exception as e:
        print(f"âŒ Error saving to S3: {e}")


# **Airflow DAG ì„¤ì •**
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2023, 3, 2),
    "retries": 1,
}
#
with DAG(
    dag_id="daily_kafka_to_s3",
    default_args=default_args,
    schedule_interval="@daily",  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,
) as dag:

    # **Kafkaë¡œ ë°ì´í„° ì „ì†¡**
    task_stream_kafka = PythonOperator(
        task_id="stream_to_kafka",
        python_callable=stream_to_kafka,
    )

    # **Kafka ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•˜ì—¬ S3ì— ì €ì¥**
    task_consume_s3 = PythonOperator(
        task_id="consume_and_save_to_s3",
        python_callable=consume_and_save_to_s3,
    )

    task_stream_kafka >> task_consume_s3  # ì‹¤í–‰ ìˆœì„œ ì§€ì •
# import datetime

# from airflow import DAG
# from airflow.operators.bash import BashOperator

# with DAG(
#     dag_id="test_dag",
#     start_date=datetime.datetime(2023, 11, 19),
#     schedule_interval="@daily",
# ) as dag:

#     step1 = BashOperator(task_id="print_date", bash_command="date")

#     step2 = BashOperator(task_id="echo", bash_command="echo 123")

#     step1 >> step2
